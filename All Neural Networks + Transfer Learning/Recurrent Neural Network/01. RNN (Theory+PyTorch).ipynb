{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN_with_PyTorch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZuG-c3AGwxv"
      },
      "source": [
        "# What is Recurring in Maths ?\n",
        "- Recurring in Maths is simply the repettition of a decimal or an expression.\n",
        "- A recurrence relation is an equation that defines a sequence based on a rule that gives the next term as a function of the previous term(s).\n",
        "- The simplest form of a recurrence relation is the case where the next term depends only on the immediately previous term.\n",
        "\n",
        "# Recurrent Neural Networks(RNN) in Deep Learning.\n",
        "- Recurrent Neural Networks (RNN) are a class of Artificial Neural Networks that can process a sequence of inputs in deep learning and retain its state while processing the next sequence of inputs.\n",
        "- Traditional neural networks will process an input and move onto the next one disregarding its sequence.\n",
        "\n",
        "# **How RNN works:**\n",
        "- <img src=\"https://www.simplilearn.com/ice9/free_resources_article_thumb/Fully_connected_Recurrent_Neural_Network.gif\" width=\"800\" height=\"350\">\n",
        "- It uses previous information to affect later ones.\n",
        "-There are 3 layers: **Input(x), Hidden(h) and output(y).**\n",
        "- In forward step when an input is recieved into the cell it gets multiplied by an **initialized weight** and then it is been **sent as an output** and then **used as an input in the next neuron**.\n",
        "- Let us visualize it using a cell level diagram.\n",
        "- <img src=\"https://blog.floydhub.com/content/images/2019/06/ezgif.com-video-to-gif.gif\" width=\"500\" height=\"350\">\n",
        "- The output of other neurons gets weight initialization as well. Except the first neuron of each layer all other neurons get inputs from **two ends 1) from the previous neuron and 2) other as an input** i.e., next element in the data.\n",
        "- This is how any current neuron will have info of its current and previous data simultaneously.\n",
        "- *The loop*: passes the input forward sequentialy, while retaining information about it.\n",
        "- Here the information possesed by a neuron is passed on to the next one as well so that the new neuron will learn new pattern by keeping previous pattern in mind.\n",
        "- This is how the sequential information in the data is preserved.\n",
        "- Thus, It performs very good when it is given any sequential data of numeric or textual format.\n",
        "\n",
        "# Types of RNN\n",
        "- There are four types of RNNs\n",
        "- <img src=\"https://static.packt-cdn.com/products/9781789536089/graphics/b2e068f5-08f8-4e4a-b56c-e29675ab0eb5.png\" width=\"1100\" height=\"400\">\n",
        "\n",
        "# Understanding the Maths behind RNN\n",
        "- Let us follow up with this diagram of **Many to Many RNN** to understand more.\n",
        "- <img src=\"https://miro.medium.com/max/2967/1*7_pAvVIMNp8h2aI4sdWdLg.png\" width=\"800\" height=\"350\">\n",
        "- Here **a0** is the initialized hidden state, It is being initialized because when start of there is no hidden state available for the first hidden cell.\n",
        "- There is also an input **x0** comming into the the cell. Now both of this gets their own **weights initialized** and the hidden state becomes **Wha.a0** and the input becomes **Whx.x0**.\n",
        "- Both of this dot products gets added and becomes **(Wha.a0+Whx.x0)**. Now after adding a **Bais(bh)** vector the whole sum becomes **(Wha.a0+Whx.x0+bh)**.\n",
        "- Now this goes into an Activation function, **TanH** is most commonly used Activation function in RNNs as it keeps the range of values in between **range(-1,1)**.\n",
        "- Now the output is **F(Wha.a0+Whx.x0+bh)**, where **