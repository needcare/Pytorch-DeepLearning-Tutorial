{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN_with_PyTorch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZuG-c3AGwxv"
      },
      "source": [
        "# What is Recurring in Maths ?\n",
        "- Recurring in Maths is simply the repettition of a decimal or an expression.\n",
        "- A recurrence relation is an equation that defines a sequence based on a rule that gives the next term as a function of the previous term(s).\n",
        "- The simplest form of a recurrence relation is the case where the next term depends only on the immediately previous term.\n",
        "\n",
        "# Recurrent Neural Networks(RNN) in Deep Learning.\n",
        "- Recurrent Neural Networks (RNN) are a class of Artificial Neural Networks that can process a sequence of inputs in deep learning and retain its state while processing the next sequence of inputs.\n",
        "- Traditional neural networks will process an input and move onto the next one disregarding its sequence.\n",
        "\n",
        "# **How RNN works:**\n",
        "- <img src=\"https://www.simplilearn.com/ice9/free_resources_article_thumb/Fully_connected_Recurrent_Neural_Network.gif\" width=\"800\" height=\"350\">\n",
        "- It uses previous information to affect later ones.\n",
        "-There are 3 layers: **Input(x), Hidden(h) and output(y).**\n",
        "- In forward step when an input is recieved into the cell it gets multiplied by an **initialized weight** and then it is been **sent as an output** and then **used as an input in the next neuron**.\n",
        "- Let us visualize it using a cell level diagram.\n",
        "- <img src=\"https://blog.floydhub.com/content/images/2019/06/ezgif.com-video-to-gif.gif\" width=\"500\" height=\"350\">\n",
        "- The output of other neurons gets weight initialization as well. Except the first neuron of each layer all other neurons get inputs from **two ends 1) from the previous neuron and 2) other as an input** i.e., next element in the data.\n",
        "- This is how any current neuron will have info of its current and previous data simultaneously.\n",
        "- *The loop*: passes the input forward sequentialy, while retaining information about it.\n",
        "- Here the information possesed by a neuron is passed on to the next one as well so that the new neuron will learn new pattern by keeping previous pattern in mind.\n",
        "- This is how the sequential information in the data is preserved.\n",
        "- Thus, It performs very good when it is given any sequential data of numeric or textual format.\n",
        "\n",
        "# Types of RNN\n",
        "- There are four types of RNNs\n",
        "- <img src=\"https://static.packt-cdn.com/products/9781789536089/graphics/b2e068f5-08f8-4e4a-b56c-e29675ab0eb5.png\" width=\"1100\" height=\"400\">\n",
        "\n",
        "# Understanding the Maths behind RNN\n",
        "- Let us follow up with this diagram of **Many to Many RNN** to understand more.\n",
        "- <img src=\"https://miro.medium.com/max/2967/1*7_pAvVIMNp8h2aI4sdWdLg.png\" width=\"800\" height=\"350\">\n",
        "- Here **a0** is the initialized hidden state, It is being initialized because when start of there is no hidden state available for the first hidden cell.\n",
        "- There is also an input **x0** comming into the the cell. Now both of this gets their own **weights initialized** and the hidden state becomes **Wha.a0** and the input becomes **Whx.x0**.\n",
        "- Both of this dot products gets added and becomes **(Wha.a0+Whx.x0)**. Now after adding a **Bais(bh)** vector the whole sum becomes **(Wha.a0+Whx.x0+bh)**.\n",
        "- Now this goes into an Activation function, **TanH** is most commonly used Activation function in RNNs as it keeps the range of values in between **range(-1,1)**.\n",
        "- Now the output is **F(Wha.a0+Whx.x0+bh)**, where **F** is Tanh activation function.\n",
        "- Let us visualize the whole above process in below GIF.\n",
        "- <img src=\"https://miro.medium.com/max/1400/1*WMnFSJHzOloFlJHU6fVN-g.gif\" width=\"800\" height=\"350\">\n",
        "- In above gif, the **new hidden state** is the output of the current hidden cell which can be regarded as **F(Wha.a0+Whx.x0+bh)** from the above math.\n",
        "- Now in the new hidden cell, this output recieved acts as an hidden state.\n",
        "- Same can be visualized in below gif.\n",
        "- <img src=\"https://miro.medium.com/max/1900/1*o-Cq5U8-tfa1_ve2Pf3nfg.gif\" width=\"900\" height=\"300\">\n",
        "- This is how RNNs are different from the ANNs, Thats all about RNNs let us proceed further to see its implementation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JBY08wO9SkC"
      },
      "source": [
        "- **NOTE:** Please refer to a more proper illustration about how RNN works before proceeding any further. The above ones are just simple tips about how different they are from the Artificial Neural Networks.\n",
        "- For Further info please go through this awesome blog:\n",
        "[RNN by Stanford](https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks), and for video watch this simple illustration [RNN Illustration Video](https://www.youtube.com/watch?v=LHXXI4-IEns&t=496s&ab_channel=TheA.I.Hacker-MichaelPhiTheA.I.Hacker-MichaelPhi)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kc1q3L_YTZxi"
      },
      "source": [
        "# Importing Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FNDDckH5GuKH"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Reay1nUUDWg"
      },
      "source": [
        "# Let us Build a RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hpj-VmVeUZVe"
      },
      "source": [
        "## Setting up **DATASET** and **HYPERPARAMETERS**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sM5JpZrQUZHI"
      },
      "source": [
        "input_size = 28 # Image of 28x28 pixels which upon flattening becomes 784.\n",
        "sequence_length = 28\n",
        "num_of_layers = 2 # Number of RNN layers we want.\n",
        "hidden_size = 256 # Number of neuron per hidden state.\n",
        "num_classes = 10 # Number of classes in the Dataset are 10.\n",
        "learning_rate = 0.001 # Speed at which we want our optimizer to optimize our solution.\n",
        "batch_size = 64 # Size of the batch that will undergo training at one step.\n",
        "epochs = 2 # Steps of training or times a forward and backward propagation is done.\n",
        "\n",
        "# Firstly Loading a Data and downloading it to the folder.\n",
        "train_dataset = datasets.MNIST(root=\"content/\",train=True,\n",
        "                               transform=transforms.ToTensor(),download=True)\n",
        "# Now setting up its properties like batchsize\n",
        "trainloader = DataLoader(d