
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Activation functions.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1AagvNb4bZsT"
      },
      "source": [
        "# Activation Functions in Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCLIHVaippje"
      },
      "source": [
        "## What is an Activation Function ?\n",
        "- Activation function is basically a reason for Activating/Trigerring or Deactivating a Neuron in a Neural Network(NN).\n",
        "- Activation function takes in sets of inputs and performs some operations on the input and converts it accordingly and gives it as an output.\n",
        "- <img src=\"https://editor.analyticsvidhya.com/uploads/37914Screenshot%20(45).png\"  width=\"600\" height=\"400\">\n",
        "- **Explanation:**\n",
        " - *W1,W2,W3...* are the weights that we initialize or can be the updated weights, *X1,X2,X3...* are the inputs that come into the neuron.\n",
        " - A Neuron can be divided into two equal parts.\n",
        " - Basically there will always be a dot product happening between the weights and input in the first half and can be denoted by *Z*.\n",
        " - Now in the second half this *Z* is being passed into the Activation Function denoted by *Alpha* in the above picture, and its output is being used as an input in the next layer.\n",
        " - Let us see some of the mostly used activation functions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7AyiX6sJ0oNK"
      },
      "source": [
        "## List of Mostly Used Activation Functions\n",
        "- **1. Linear**\n",
        "- **2. Identity**\n",
        "- **3. Binary step(Heaviside Function)**\n",
        "- **4. Signum**\n",
        "- **5. ReLU (Rectified Linear Unit)**\n",
        "- **6. PReLU (Parametric Rectified Linear Unit)**\n",
        "- **7. LeakyReLU**\n",
        "- **8. ELU (Exponential Linear Unit)**\n",
        "- **9. Sigmoid**\n",
        "- **10. Tanh**\n",
        "- **11. ArcTan**\n",
        "- **12. Swish**\n",
        "- **13. Softmax**\n",
        "- **14. SoftPlus**\n",
        "- **15. Sinusoid**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9G-_TQTV6ptC"
      },
      "source": [
        "- Pretty Big list haa...Let us Explore them one by one and try to visualize them through their behaviour to different set of values. Also we will look at some maths behind it!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nMqI8GMlnC7t"
      },
      "source": [
        "# Importing Packages that visualize our data and its respective transformations\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn"
      ],