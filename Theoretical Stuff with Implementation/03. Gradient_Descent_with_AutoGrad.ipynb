
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Gradient Descent with AutoGrad.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUI4tWXQqKUP"
      },
      "source": [
        "# Importing PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fU5-yF8Kp-u4"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn # It has almost all Layers and Activation functions.\n",
        "import torch.optim as optim # It has a lot of Optimizers.\n",
        "import torch.nn.functional as F # It has some Activation Functions as well.\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QURLW01FTA5P"
      },
      "source": [
        "# Importing Other Important Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Yf59ehIrfa7"
      },
      "source": [
        "- Now let us try to see the underlying algorithm of how Gradient Descent works from Scratch.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHyh-hT0UFNW"
      },
      "source": [
        "# What is Gradient Descent ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQ2jGF5wWWul"
      },
      "source": [
        "- let us suppose we have **X=5 , Y_true = 10**.\n",
        "- Now the Objective is to find the value of **'W'** such that **W*X=Y_true**.\n",
        "- Now applying it here is pretty easy where we divide Y_true with X to give 10/5=2 as the value of **W**.\n",
        "- But in real world the equation is not that easy to solve and need **Exact Solution** to solve the equations.\n",
        "- There are many **Numerical Methods** in Mathematics to solve this kind of **Exact equations** of which one is **Gradient Descent** Algorithm.\n",
        "- **Gradient Descent:**Gradient descent is an optimization algorithm used to minimize some function by iteratively moving in the direction of steepest descent as defined by the negative of the gradient. In Deep learning, we use gradient descent to update the parameters of our model.\n",
        "-- <img src='https://lucidar.me/en/neural-networks/files/gradient-overview.png' width=400 />\n",
        "- Here Target minimum is known as **Global Minima** which is the lowest possible error between the true and predicted value.\n",
        "- At this Global Minima whatever weight it has applied to reach the point would be considered as an optium Weight for prediction.\n",
        "- We will initially use *Numpy for understanding it from depth*.\n",
        "- Later we will use **AUTOGRAD** for further simplification.\n",
        "- As we are gonna watch two implementations let us move forward with the first one.\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJZukQMEsvyZ"
      },
      "source": [
        "# Gradient Descent using Numpy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5RyPHsiaBtI"
      },
      "source": [
        "## Initializing the Data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNUhoIBJkSdO"
      },
      "source": [
        "# Firstly let us decide our Predictor->X and Target->Y values respectively.\n",
        "X = np.array([1,2,3,4])\n",
        "Y = np.array([3,6,9,12])"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOEaovnWXwIy"
      },
      "source": [
        "- The Equation of our forward step is **Y=W*X,** and clearly the value of **W** is **3** which upon multiplied to **X** will give us **Y**. \n",
        "- Let us intiate **W** to zero and see if we can iteratively reach to the original solution by reducing the weights."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vbcr12NlkSg2"
      },
      "source": [
        "# Let us firstly set weight to zero\n",
        "W=0"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjnHA2iBZCHb"
      },
      "source": [
        "- Let us make our forward function as follows.\n",
        "- Also will try to make a loss function which in our case would be **Mean Squarred error.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Tmo15LyaG2X"
      },
      "source": [
        "## Creating Forward function and Loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqAj0xOmkSkJ"